<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>

<title>Overview</title>





<style type="text/css">
body, td {
   font-family: sans-serif;
   background-color: white;
   font-size: 13px;
}

body {
  max-width: 800px;
  margin: auto;
  padding: 1em;
  line-height: 20px;
}

tt, code, pre {
   font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}

h1 {
   font-size:2.2em;
}

h2 {
   font-size:1.8em;
}

h3 {
   font-size:1.4em;
}

h4 {
   font-size:1.0em;
}

h5 {
   font-size:0.9em;
}

h6 {
   font-size:0.8em;
}

a:visited {
   color: rgb(50%, 0%, 50%);
}

pre, img {
  max-width: 100%;
}

pre code {
   display: block; padding: 0.5em;
}

code {
  font-size: 92%;
  border: 1px solid #ccc;
}

code[class] {
  background-color: #F8F8F8;
}

table, td, th {
  border: none;
}

blockquote {
   color:#666666;
   margin:0;
   padding-left: 1em;
   border-left: 0.5em #EEE solid;
}

hr {
   height: 0px;
   border-bottom: none;
   border-top-width: thin;
   border-top-style: dotted;
   border-top-color: #999999;
}

@media print {
   * {
      background: transparent !important;
      color: black !important;
      filter:none !important;
      -ms-filter: none !important;
   }

   body {
      font-size:12pt;
      max-width:100%;
   }

   a, a:visited {
      text-decoration: underline;
   }

   hr {
      visibility: hidden;
      page-break-before: always;
   }

   pre, blockquote {
      padding-right: 1em;
      page-break-inside: avoid;
   }

   tr, img {
      page-break-inside: avoid;
   }

   img {
      max-width: 100% !important;
   }

   @page :left {
      margin: 15mm 20mm 15mm 10mm;
   }

   @page :right {
      margin: 15mm 10mm 15mm 20mm;
   }

   p, h2, h3 {
      orphans: 3; widows: 3;
   }

   h2, h3 {
      page-break-after: avoid;
   }
}
</style>



</head>

<body>
<h2>Overview</h2>

<p>This section of the analysis calculates the differences between the GTI cropland percentages and those from the various landcover products. It draws from results generated by the <a href="https://github.com/PrincetonUniversity/SAcropland/blob/master/vignettes/sa-cropland-analysis.Rmd">cropland pre-processing</a>. (Note to self here: I want to have a transportable link between chapters here, rather than a direct link to the other RMD document in 
github).</p>

<h2>Analyses</h2>

<h3>Sidebar</h3>

<p>I am just learning how to use Rmarkdown with Rmd files to document analyses, primarily analyses that are more interactive in nature and notrepeated many times. Code that will be re-used multiple times will go into a library. </p>

<p>One things I am noticing is that the knit process for compiling the notebook does not find functions within code chunks unless the libraries are called within the code chunks themselves. It seems like each code chunk is its own environment.  The solution to this is to do something like <code>raster::raster()</code> when you want a function, or run this where chunks are set to {r, eval = FALSE}, and just run the code chunk as if you were in a normal R script working interactively.  I have already set up the library for SAcropland to call the packages its needs so they are loaded up in the global environment. </p>

<h3>Load in data</h3>

<ol>
<li>Set up paths</li>
</ol>

<pre><code class="r">library(lmisc)
library(Hmisc)
library(raster)
library(SAcropland)
p_root &lt;- proj_root(&quot;SAcropland&quot;)
p_data &lt;- full_path(p_root, &quot;SAcropland/external/ext_data/&quot;)
</code></pre>

<ol>
<li>Load cropland grids 

<ul>
<li>Derived from global landcover products</li>
</ul></li>
</ol>

<pre><code class="r">salc &lt;- raster(full_path(p_data, &quot;sa_ag_masked.tif&quot;)) * 100
globsa_list_sum &lt;- lapply(full_path(p_data, dir(p_data, &quot;globSA*.*1kmrect_sum.tif&quot;)[c(2:3, 1)]), raster)
modsa_list_sum &lt;- lapply(full_path(p_data, dir(p_data, &quot;mod_1*.*1kmrect_sum.tif&quot;)[c(2:3, 1)]), raster)
glcsa &lt;- raster(full_path(p_data, &quot;glcsa_masked.tif&quot;))
geow &lt;- raster(full_path(p_data, &quot;geowikisa_masked.tif&quot;))
lclist &lt;- c(salc, globsa_list_sum, modsa_list_sum, glcsa, geow)  # into list
names(lclist) &lt;- c(&quot;sa30&quot;, &quot;globmin&quot;, &quot;globmu&quot;, &quot;globmax&quot;, &quot;modmin&quot;, &quot;modmu&quot;, &quot;modmax&quot;, &quot;glcsh&quot;, &quot;geow&quot;)
</code></pre>

<ul>
<li>SA landcover data</li>
</ul>

<pre><code class="r">load(full_path(p_data, &quot;MZshapes.Rdata&quot;))  
paths &lt;- full_path(p_data, dir(p_data, paste(&quot;cover*.*sum_mask*&quot;, sep = &quot;&quot;)))
gti &lt;- lapply(paths, raster)
</code></pre>

<p>[[back to top]][Overview]</p>

<h3>Calculate differences at different levels of aggregation</h3>

<p>This steps draws on functions I wrote to calculate the actual and absolute differences between a given &quot;master&quot; grid and lists of other grids. This comparison is done at multiple aggregations, ranging from 1-600 km resolution cropland averages. The lists have two levels of nesting, here the upper provides the resolution, the inner provides the different landcover datasets. </p>

<pre><code class="r"># Mean landcover between 2007 and 2011
gti2 &lt;- c(gti, calc(stack(gti), mean))
names(gti2) &lt;- c(&quot;gti2007&quot;, &quot;gti2011&quot;, &quot;gtimu&quot;)

# Aggregate rasters
fact &lt;- c(5, 10, 20, 50, 100, 200)
lc_agg &lt;- aggregate_rast_list(fact, lclist)   # landcover rasters 
</code></pre>

<pre><code>## [1] &quot;aggregating by factor of 5&quot;
## [1] &quot;aggregating by factor of 10&quot;
## [1] &quot;aggregating by factor of 20&quot;
## [1] &quot;aggregating by factor of 50&quot;
## [1] &quot;aggregating by factor of 100&quot;
## [1] &quot;aggregating by factor of 200&quot;
</code></pre>

<pre><code class="r">gti_agg &lt;- aggregate_rast_list(fact, gti2)  # GTI rasters
</code></pre>

<pre><code>## [1] &quot;aggregating by factor of 5&quot;
## [1] &quot;aggregating by factor of 10&quot;
## [1] &quot;aggregating by factor of 20&quot;
## [1] &quot;aggregating by factor of 50&quot;
## [1] &quot;aggregating by factor of 100&quot;
## [1] &quot;aggregating by factor of 200&quot;
</code></pre>

<pre><code class="r"># Actual differences
diff_list_act &lt;- lapply(1:length(gti2), function(x) {
  print(paste(&quot;processing&quot;, x))
  diffr &lt;- diff_rast_list(1:length(lc_agg), subtractor = lc_agg, subtractee = gti_agg, whichdiff = x)
})
names(diff_list_act) &lt;- c(&quot;d2007&quot;, &quot;d2011&quot;, &quot;dmu&quot;)  

# Absolute differences
diff_list_abs &lt;- lapply(1:length(gti2), function(x) {
  print(paste(&quot;processing&quot;, x))
  diffr &lt;- diff_rast_list(1:length(lc_agg), subtractor = lc_agg, subtractee = gti_agg, whichdiff = x, 
                            abs = TRUE)
})
names(diff_list_abs) &lt;- c(&quot;d2007&quot;, &quot;d2011&quot;, &quot;dmu&quot;)  

# Between 2007 and 2011 GTI
diff_list_gti &lt;- diff_rast_list(1:7, gti_agg, gti_agg, 3)

save(diff_list_act, file = full_path(p_data, &quot;diff_grid_act.rda&quot;))
save(diff_list_abs, file = full_path(p_data, &quot;diff_grid_abs.rda&quot;))
</code></pre>

<p>[[back to top]][Overview]</p>

<p>14/11/14 - Here I ran into the issue of needing to weight quantiles according to how many pixels went into values in aggregated pixels, otherwise statistics are biased. I explored here different ways of doing this. I am document this here before committing changes so that I can roll this back and look at what I did as needed. I compared original methods statistics with two ways of applying weightings.  </p>

<p>First test method 1. Create a mask for nodata areas, then take aggregated pixels for both the GTI data and the difference grids, disaggregate them, and crop them to the extent of the NA grid. The GTI grid is &quot;cut&quot; into bins, which is less clunky then the previous incarnation.  We then loop through the lists (here I test on just the geowiki differences at 20 km aggregation) by the 2007, 2011, and mean of those two, and extract just the bin I want (now a class), set 0s to NA, multiply by the corresponding difference grid, pooling differences from across all 3 gti version, and then I run box_stats on that. <em>Note: if I run this on the original aggregated geowiki dataset, without disaggregating, I get the same answer as the final statistics for geowiki at 20 km resolution.</em>  </p>

<pre><code class="r">load(full_path(p_data, &quot;diff_grid_act.rda&quot;))  # here we&#39;ll cheat by loading in the diff grid w/o re-running
namask &lt;- !is.na(gti2$gtimu)
area_wgts &lt;- aggregate(namask, fact = 20, fun = sum)
namask[namask == 0] &lt;- NA

binv &lt;- seq(0, 100, 5)  # cropland bins
rl &lt;- lapply(1:3, function(x) {
  r &lt;- disaggregate(gti_agg$f20[[x]]$geow, fact = 20)
  r &lt;- raster::mask(crop(r, namask), namask)
  rcl &lt;- cut(r, breaks = binv, include.lowest = TRUE)
})

r3l &lt;- lapply(1:3, function(x) {
  r &lt;- disaggregate(diff_list_act[[x]]$f20$geow, fact = 20)
  r &lt;- raster::mask(crop(r, namask), namask)
})

t1 &lt;- do.call(rbind, lapply(1:(length(binv) - 1), function(x) {
  print(x)
  l1 &lt;- unlist(lapply(1:length(rl), function(k) {
    rs &lt;- rl[[k]] == x
    rs[rs == 0] &lt;- NA
    o &lt;- rs * r3l[[k]]
    getValues(o)
  }))
  box_stats(l1)
}))
</code></pre>

<pre><code>## [1] 1
## [1] 2
## [1] 3
## [1] 4
## [1] 5
## [1] 6
## [1] 7
## [1] 8
## [1] 9
## [1] 10
## [1] 11
## [1] 12
## [1] 13
## [1] 14
## [1] 15
## [1] 16
## [1] 17
## [1] 18
## [1] 19
## [1] 20
</code></pre>

<p>In the next version, I don&#39;t do any disaggregation, but I aggregate the NA mask dataset by summing, to get the count of pixels that went into each pixel at each level of aggregation (done in the previous chunk.  This then effectively becomes a weighting raster, which I extract for each aggregated difference.  I then use this as the weights within the Hmisc package.  The result is the same as the first version, but much faster. </p>

<pre><code class="r">p &lt;- c(0.025, 0.25, 0.5, 0.75, 0.975)
rl &lt;- lapply(gti_agg$f20, function(x) cut(x, breaks = binv, include.lowest = TRUE))
r3l &lt;- lapply(1:3, function(x) diff_list_act[[x]]$f20$geow)
t2 &lt;- do.call(rbind, lapply(1:(length(binv) - 1), function(x) {
  print(x)
  l1 &lt;- do.call(rbind, lapply(1:length(rl), function(k) {
    rs &lt;- rl[[k]] == x
    rs[rs == 0] &lt;- NA
    o &lt;- rs * r3l[[k]]
    w &lt;- area_wgts * rs
    oo &lt;- cbind(getValues(o), getValues(w))
  }))
  l12 &lt;- l1[!is.na(l1[, 1]), ]
  if(nrow(l12) &gt; 0) {
    qs &lt;- c(nrow(l12), wtd.quantile(l12[, 1], weights = l12[, 2], probs = p, na.rm = FALSE), 
          wtd.mean(l12[, 1], weights = l12[, 2], na.rm = FALSE))
  } else {
    qs &lt;- rep(NA, 6)
  }
  qs
}))
</code></pre>

<pre><code>## [1] 1
## [1] 2
## [1] 3
## [1] 4
## [1] 5
## [1] 6
## [1] 7
## [1] 8
## [1] 9
## [1] 10
## [1] 11
## [1] 12
## [1] 13
## [1] 14
## [1] 15
## [1] 16
## [1] 17
## [1] 18
## [1] 19
## [1] 20
</code></pre>

<pre><code>## Warning: number of columns of result is not a multiple of vector length
## (arg 19)
</code></pre>

<pre><code class="r">t2; t1
</code></pre>

<pre><code>##              2.5%   25.0%    50.0%   75.0%  97.5%        
##  [1,] 6783 -25.34   0.000  0.01045  0.3257  2.502 -1.9087
##  [2,]  747 -24.46  -3.749  0.40069  3.1779  6.601 -1.6741
##  [3,]  470 -21.33  -2.813  2.17269  5.9178 10.114  0.2179
##  [4,]  295 -30.39  -5.681  1.62696  7.5539 13.490 -0.7013
##  [5,]  231 -23.50  -1.464  3.88993  8.9758 16.078  2.1648
##  [6,]  197 -31.89  -5.959  2.11154  6.9211 20.320 -0.4453
##  [7,]  202 -21.87  -5.686  0.73917  5.0722 16.751 -0.4029
##  [8,]  190 -17.74  -6.863 -0.19939  2.8850 14.302 -1.3822
##  [9,]  166 -17.18  -6.531 -1.65756  2.8452 19.659 -0.6376
## [10,]  134 -26.96  -8.826 -3.63513  0.3451  8.190 -4.9481
## [11,]  134 -36.31 -10.672 -3.47969  3.9276 22.395 -3.0610
## [12,]   72 -25.00 -10.507 -5.25925  0.1580  8.462 -5.9035
## [13,]   58 -22.96  -8.736 -2.35066  6.4266 16.018 -2.9588
## [14,]   53 -18.59  -7.615  0.03559  3.6608 32.910  1.9173
## [15,]   32 -17.76 -14.276 -4.71859 19.3683 36.949  0.2815
## [16,]   24 -15.88 -14.139 -9.54667 -8.7380 43.166 -5.1449
## [17,]   20 -14.54 -14.222 -3.87587 -1.9974 44.063 -3.4314
## [18,]   14 -11.38  -8.938 -5.63328 -4.4561 -3.638 -6.9709
## [19,]   NA     NA      NA       NA      NA     NA      NA
## [20,]   NA     NA      NA       NA      NA     NA      NA
</code></pre>

<pre><code>##         2.5%     25%      50%     75%  97.5%      mu
##  [1,] -25.34   0.000  0.01045  0.3257  2.502 -1.9087
##  [2,] -24.46  -3.749  0.40069  3.1779  6.601 -1.6741
##  [3,] -21.33  -2.813  2.17269  5.9178 10.114  0.2179
##  [4,] -30.39  -5.681  1.62696  7.5539 13.490 -0.7013
##  [5,] -23.50  -1.464  3.88993  8.9758 16.078  2.1648
##  [6,] -31.89  -5.959  2.11154  6.9211 20.320 -0.4453
##  [7,] -21.87  -5.686  0.73917  5.0722 16.751 -0.4029
##  [8,] -17.74  -6.863 -0.19939  2.8850 14.302 -1.3822
##  [9,] -17.18  -6.531 -1.65756  2.8452 19.659 -0.6376
## [10,] -26.96  -8.826 -3.63513  0.3451  8.190 -4.9481
## [11,] -36.31 -10.672 -3.47969  3.9276 22.395 -3.0610
## [12,] -25.00 -10.507 -5.25925  0.1580  8.462 -5.9035
## [13,] -22.96  -8.736 -2.35066  6.4266 16.018 -2.9588
## [14,] -18.59  -7.615  0.03559  3.6608 32.910  1.9173
## [15,] -17.76 -14.276 -4.71859 19.3683 36.949  0.2815
## [16,] -15.88 -14.139 -9.54667 -8.7380 43.166 -5.1449
## [17,] -14.54 -14.222 -3.87587 -1.9974 44.063 -3.4314
## [18,] -11.38  -8.938 -5.63328 -4.4561 -3.638 -6.9709
## [19,]     NA      NA       NA      NA     NA     NaN
## [20,]     NA      NA       NA      NA     NA     NaN
</code></pre>

<h3>Bias as function of cropcover</h3>

<p>Values from the bias rasters calculated in the previous section are binned by percent cropland cover.  There are a couple of tricky aspects here, related to choosing which cropland map provides the bins. The 2011 GTI mapset more closely corresponds to the MODIS and GLC-SHARE datasets in time, whereas GlobCover and the SA landcover dataset fall closer to or are between the 2007 mapset. <strong>Wayforward</strong>: Calculate whether there is any bias between the 2007 and 2011 cropland estimates by GTI. And also see how much difference exists in bias estimates <em>as a function of cropcover</em> when 2011 GTI provides the bin estimates, and the difference rasters are calculated against different GTI datasets (2007, the mean of 2007 and 2011, and 2011). </p>

<p>Another complication is the landcover classes to include in the GTI data.  Originally I had the subsistence class added in, but I am now only going to use the no-subsistence set. The no-subsistence set masks out all pixels with &gt;0% subsistence farming coverage. I have also masked out all areas having forestry plantations, which are included in the GLC-SHARE dataset.  </p>

<pre><code class="r">binv &lt;- seq(0, 100, 5)  # cropland bins

# GTI versus GTI (to measure difference in the different permutations of GTI)
extract_bin_values(gti_agg, &quot;gti2011&quot;, diff_list_gti, binv, full_path(p_data, &quot;gti_diff_bin&quot;)) 

# Actual landcover differences relative to GTI
nmrt &lt;- c(&quot;2007_bin&quot;, &quot;2011_bin&quot;, &quot;mu_bin&quot;)
mind &lt;- c(&quot;gti2007&quot;, &quot;gti2011&quot;, &quot;gtimu&quot;)
dind &lt;- c(&quot;d2007&quot;, &quot;d2011&quot;, &quot;dmu&quot;)
onm &lt;- paste(&quot;act&quot;, nmrt, sep = &quot;_&quot;)
lapply(1:3, function(x) {
  extract_bin_values(gti_agg, mind[x], diff_list_act[[dind[x]]], binv, full_path(p_data, onm[x]))
})

# Absolute landcover differences
onm &lt;- paste(&quot;abs&quot;, nmrt, sep = &quot;_&quot;)# c(&quot;abs_2011_ns_bin&quot;, &quot;abs_mu_ns_bin&quot;, &quot;abs_2007_ns_bin&quot;)
lapply(1:3, function(x) {
  extract_bin_values(gti_agg, mind[x], diff_list_abs[[dind[x]]], binv, full_path(p_data, onm[x]))
})
rm(diff_list_abs, diff_list_act, diff_list_gti)
</code></pre>

<p>[[back to top]][Overview]</p>

<h3>Calculate biases</h3>

<h4>For GTI data</h4>

<pre><code class="r">load(full_path(p_data, &quot;gti_diff_bin.rda&quot;))
gti_bias &lt;- bin_stats(bin_val_list, fun.list = list(mean), binv)
</code></pre>

<h4>For difference rasters</h4>

<p>For this, we will have to reshape the difference lists so that we end up with a list that pools biases by cropcover bin across the 3 GTI permutations, but separately for each scale and sensor, before calculating statistics. This is to account for as much uncertainty as possible (e.g. for temporal mismatches between landcover products and GTI). MODIS and GlobCover sets also need to be pooled across the high, medium, and low cover estimates.  </p>

<pre><code class="r"># Read in and reshape actual differences
nms &lt;- full_path(p_data, paste(paste(&quot;act&quot;, nmrt, sep = &quot;_&quot;), &quot;.rda&quot;, sep = &quot;&quot;))
bin_act &lt;- lapply(nms, function(x) {
  load(x)
  bin_val_list
})
names(bin_act) &lt;- paste(&quot;act&quot;, nmrt, sep = &quot;_&quot;)

fact_all &lt;- c(1, fact)# full aggregation levels
lev_vec &lt;- paste(&quot;f&quot;, fact_all, sep = &quot;&quot;)  
resh &lt;- reshape_diff_list(bin_act, names(lclist), lev_vec, length(binv) - 1)  # reshape
resh_modglob &lt;- reshape_reshape_list(resh, c(&quot;mod&quot;, &quot;glob&quot;), lev_vec, length(binv) - 1)  # pool modis and glob
resh_out_act &lt;- c(resh[c(&quot;sa30&quot;, &quot;glcsh&quot;, &quot;geow&quot;)], resh_modglob)  # Recombine final reshapes into output list
rm(bin_act)

# Reshape absolute differences
nms &lt;- full_path(p_data, paste(paste(&quot;abs&quot;, nmrt, sep = &quot;_&quot;), &quot;.rda&quot;, sep = &quot;&quot;))
bin_abs &lt;- lapply(nms, function(x) {
  load(x)
  bin_val_list
})
names(bin_abs) &lt;- paste(&quot;bs&quot;, nmrt, sep = &quot;_&quot;)

resh &lt;- reshape_diff_list(bin_abs, names(lclist), lev_vec, length(binv) - 1)  # Reshape
resh_modglob &lt;- reshape_reshape_list(resh, c(&quot;mod&quot;, &quot;glob&quot;), lev_vec, length(binv) - 1)  # pool modis and glob
resh_out_abs &lt;- c(resh[c(&quot;sa30&quot;, &quot;glcsh&quot;, &quot;geow&quot;)], resh_modglob)  # Recombine final reshapes into output list
rm(resh, resh_modglob, bin_abs)
</code></pre>

<p>A note: I did checks here to make sure the reshaping was correct, namely by manually pulling out a particular bin from a particular sensor and scale and calculating the mean from that and comparing to the mean from the corresponding part of the reshaped lists. This includes checking the pooled MODIS datasets. 
[[back to top]][Overview]</p>

<h3>Calculate statistics</h3>

<p>Now to calculate final statistics per landcover &quot;density&quot; bin (mean, median, 5th, 25th, 75th and 95th percentiles, etc) for each landcover set at each level of aggregation. I am relying on the box_stats function from my lmisc package to generate the necessary statistics for this.</p>

<pre><code class="r">act_stats &lt;- lapply(resh_out_act, function(x) lapply(x, function(y) {
  t(sapply(y, function(z) c(&quot;n&quot; = length(z), box_stats(z))))
}))
abs_stats &lt;- lapply(resh_out_abs, function(x) lapply(x, function(y) {
  t(sapply(y, function(z) c(&quot;n&quot; = length(z), box_stats(z))))
}))
</code></pre>

<p>Save bias statistics and clean up transitional files</p>

<pre><code class="r">save(gti_bias, act_stats, abs_stats, file = full_path(p_data, &quot;bias_stats.rda&quot;))
for(i in c(&quot;act&quot;, &quot;abs&quot;)) file.remove(full_path(p_data, paste(paste(i, nmrt, sep = &quot;_&quot;), &quot;.rda&quot;, sep = &quot;&quot;)))
</code></pre>

<h2>Next steps</h2>

<p>Plot bias results.  </p>

<p>[[back to top]][Overview]</p>

</body>

</html>
